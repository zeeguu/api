# Changes Made on 2025-11-04

## Summary
Fixed critical API deadlock/freezing issues and optimized performance for the Zeeguu API deployment.

---

## 1. Fixed Critical Deadlock Issue

### Problem
API was freezing after multiple translation requests for user mlun@itu.dk. Symptoms:
- Workers consuming 394-399% CPU
- Translation requests never reaching Flask (no logs)
- Database errors: `Commands out of sync; you can't run this command now`
- `Lost connection to server during query`

### Root Cause
The `--preload` flag in Gunicorn caused all 4 workers to share the same database connection pool from the parent process, leading to connection corruption and deadlocks when multiple workers tried to use connections concurrently.

### Solution
**File:** `Dockerfile.gunicorn` (renamed to `Dockerfile` on production)

Removed the `--preload` flag from Gunicorn command:

```dockerfile
# Before (line 69):
# --preload loads application before forking (shares Stanza models in memory)

# After (lines 69-70):
# NOTE: --preload removed - causes database connection sharing across workers leading to deadlocks
# Each worker now initializes its own DB connections and Stanza models (slight memory overhead but safe)
```

**Result:** Each worker now initializes its own clean database connections, preventing deadlocks.

---

## 2. Implemented Tokenization Caching

### Problem
Homepage was freezing on refresh due to 15+ concurrent `/user_article_summary` requests all performing CPU-intensive Stanza tokenization, saturating workers at 60-101% CPU each.

### Solution
Created a separate tokenization cache table to avoid bloating the main article table.

#### Database Migration
**File:** `tools/migrations/25-11-04-a--add_cached_tokenization.sql`

```sql
CREATE TABLE IF NOT EXISTS article_tokenization_cache (
    article_id INT PRIMARY KEY,
    tokenized_summary MEDIUMTEXT DEFAULT NULL COMMENT 'JSON-encoded tokenized summary (Stanza tokens)',
    tokenized_title TEXT DEFAULT NULL COMMENT 'JSON-encoded tokenized title (Stanza tokens)',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (article_id) REFERENCES article(id) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```

#### New Model
**File:** `zeeguu/core/model/article_tokenization_cache.py` (new file)

Created `ArticleTokenizationCache` model with 1:1 relationship to Article table, with methods:
- `find_or_create(session, article)` - Get or create cache entry
- `get_for_article(session, article_id)` - Retrieve cache for article

#### Updated Article Model
**File:** `zeeguu/core/model/article.py` (lines 130-136)

Added 1:1 relationship to tokenization cache:

```python
tokenization_cache = relationship(
    "ArticleTokenizationCache",
    back_populates="article",
    uselist=False,  # 1:1 relationship
    cascade="all, delete-orphan",
)
```

#### Updated User Article Summary
**File:** `zeeguu/core/model/user_article.py` (lines 518-583)

Modified `user_article_summary_info()` to:
1. Check for cached tokenization first
2. Use cached tokens if available
3. Tokenize and cache if not found
4. Commit cache to database

#### Model Registration
**File:** `zeeguu/core/model/__init__.py` (line 11)

Added import: `from .article_tokenization_cache import ArticleTokenizationCache`

**Performance Improvement:** 9.7x faster (89.7% reduction in processing time)

---

## 3. Implemented Pre-Tokenization at Crawl Time

### Problem
Users experienced slow first-time loads when tokenization wasn't cached.

### Solution
**File:** `zeeguu/core/content_retriever/article_downloader.py`

Added `_cache_article_tokenization()` function (lines 71-103):
- Pre-tokenizes article title and summary during feed crawling
- Stores in cache table immediately
- Users never experience slow first-time tokenization

Called from `download_feed_item()` at line 490:
```python
session.add(new_article)
_cache_article_tokenization(new_article, session)
```

---

## 4. Fixed Stanza Tokenizer Race Condition

### Problem
Multiple threads simultaneously loading the same Stanza model, causing initialization conflicts.

### Solution
**File:** `zeeguu/core/tokenization/stanza_tokenizer.py`

Added thread-safe model loading (lines 9, 32):
```python
import threading  # Added

class StanzaTokenizer(ZeeguuTokenizer):
    _PIPELINE_LOAD_LOCK = threading.Lock()  # Added

    # Use double-checked locking pattern in __init__
    if key not in StanzaTokenizer.CACHED_NLP_PIPELINES:
        with StanzaTokenizer._PIPELINE_LOAD_LOCK:
            if key not in StanzaTokenizer.CACHED_NLP_PIPELINES:
                # Load model safely
```

---

## 5. Improved Connection Pool Configuration

### Problem
Default connection pool of 5 was too small for concurrent requests.

### Solution
**File:** `zeeguu/api/app.py` (lines 69-79)

Increased connection pool size with MySQL-specific configuration:

```python
if "mysql" in app.config["SQLALCHEMY_DATABASE_URI"].lower():
    app.config["SQLALCHEMY_ENGINE_OPTIONS"] = {
        "pool_size": 10,  # Core pool size (up from default 5)
        "max_overflow": 20,  # Additional connections (up from default 10)
        "pool_recycle": 3600,  # Recycle connections after 1 hour
        "pool_pre_ping": True,  # Verify connections before using them
    }
```

**Total pool:** 30 connections available

---

## 6. Implemented Stanza Tokenizer Preloading

### Problem
Stanza models loading during requests (20-480ms blocking time).

### Solution
**File:** `zeeguu/api/app.py` (lines 215-240)

Added preloading at application startup:

```python
with app.app_context():
    if app.config.get("PRELOAD_STANZA", True):
        warning("*** Preloading Stanza tokenizers...")
        start_time = time.time()
        from zeeguu.core.model import Language
        from zeeguu.core.tokenization import get_tokenizer, TOKENIZER_MODEL

        language_codes = Language.CODES_OF_LANGUAGES_THAT_CAN_BE_LEARNED

        for lang_code in language_codes:
            try:
                language = Language.find_or_create(lang_code)
                tokenizer = get_tokenizer(language, TOKENIZER_MODEL)
                tokenizer.tokenize_text("test")
                warning(f"*** Preloaded Stanza tokenizer for {lang_code}")
            except Exception as e:
                warning(f"*** Failed to preload Stanza for {lang_code}: {e}")

        elapsed = time.time() - start_time
        warning(f"*** Stanza tokenizers preloaded for {len(language_codes)} languages in {elapsed:.2f}s")
```

**Result:** ~0.65s startup time to preload 15 language models

---

## 7. Fixed Elasticsearch OOM Crashes

### Problem
Elasticsearch crashing with exit code 137 (Out of Memory), constant garbage collection overhead.

### Solution
**File:** `docker-compose.yml` (on production server)

Added memory limits and disabled unnecessary features:

```yaml
elasticsearch_v8:
  image: elasticsearch:8.12.2
  environment:
    - discovery.type=single-node
    - xpack.security.enabled=false
    - "ES_JAVA_OPTS=-Xms512m -Xmx512m"  # Added: Limit heap to 512MB
    - xpack.ml.enabled=false  # Added: Disable machine learning
  mem_limit: 1g  # Added: Total container memory limit
  volumes:
    - ${ZEEGUU_DATA_FOLDER}/elasticsearch_db_v8/data:/usr/share/elasticsearch/data
  networks:
    - zeeguu_backend
  restart: unless-stopped
```

**Note:** kNN vector search (used for embeddings) works without ML module in ES 8.x

---

## 8. Disabled Wordstats Preloading

### Problem
Wordstats preloading slowed down startup (happening 4 times, once per worker).

### Solution
**Production config change:** Set `PRELOAD_WORDSTATS=False` in `api.cfg`

Wordstats now uses lazy loading (loads on first use instead of startup).

---

## 9. Migration to Gunicorn from Apache

### Problem
Apache mod_wsgi configuration was complex and had threading issues.

### Solution
**File:** `Dockerfile.gunicorn` (renamed to `Dockerfile`)

Created simplified Dockerfile using Gunicorn:
- 68 lines vs 126 lines (Apache version)
- Removed all Apache dependencies
- Added gunicorn to requirements
- Configuration: 4 workers × 15 threads = 60 concurrent handlers
- 300 second (5 minute) request timeout

**Benefits:**
- Simpler configuration
- Better performance
- Easier debugging
- No mod_wsgi complexity

---

## Configuration Changes

### Production Server
1. **Docker image rebuilt** with Gunicorn and all fixes
2. **Migration applied:** `25-11-04-a--add_cached_tokenization.sql`
3. **Config updated:** `PRELOAD_WORDSTATS=False` in `api.cfg`
4. **Elasticsearch restarted** with memory limits
5. **All containers restarted** with new configurations

### Volume Mappings (Verified)
- `/home/zeeguu/data:/zeeguu-data` (audio files, data)
- `/home/zeeguu/src/api:/Zeeguu-API` (source code)
- `/home/zeeguu/ops/running/api/api.cfg:/Zeeguu-API/api.cfg` (config)
- `/home/zeeguu/ops/running/api/fmd.cfg:/Zeeguu-API/fmd.cfg` (config)
- `/home/zeeguu/ops/running/api/lu-mir-zeeguu-credentials.json` (credentials)
- `/home/zeeguu/ops/running/api/yt_cookies.txt` (YouTube cookies)
- `/home/zeeguu/ops/deployments/stiri-simple.github.io` (deployment)

---

## Results

### Performance Improvements
- **Homepage loading:** 9.7x faster with tokenization caching (89.7% reduction)
- **Translation stability:** No more deadlocks or freezing
- **Startup time:** Faster without wordstats preloading
- **Elasticsearch:** Stable, no more OOM crashes
- **CPU usage:** Normal levels, no more sustained 300%+ spikes

### Stability Improvements
- ✅ Database connection corruption fixed
- ✅ Race conditions eliminated
- ✅ Worker deadlocks resolved
- ✅ Elasticsearch memory issues resolved
- ✅ Thread-safe model loading

### Testing
- ✅ Multiple translations with mlun@itu.dk - no freezing
- ✅ Homepage refreshes - fast and stable
- ✅ Elasticsearch queries working
- ✅ All 176 tests passing

---

## Files Modified

### Core Application Files
1. `zeeguu/api/app.py` - Connection pool, Stanza preloading
2. `zeeguu/core/tokenization/stanza_tokenizer.py` - Thread-safe loading
3. `zeeguu/core/model/user_article.py` - Tokenization caching
4. `zeeguu/core/model/article.py` - Tokenization cache relationship
5. `zeeguu/core/model/__init__.py` - Model import
6. `zeeguu/core/content_retriever/article_downloader.py` - Pre-tokenization

### New Files
1. `zeeguu/core/model/article_tokenization_cache.py` - Cache model
2. `tools/migrations/25-11-04-a--add_cached_tokenization.sql` - Database migration

### Infrastructure Files
1. `Dockerfile.gunicorn` (renamed to `Dockerfile`) - Gunicorn configuration
2. `docker-compose.yml` (on production) - Elasticsearch memory limits

### Configuration Files (on production server)
1. `api.cfg` - Added `PRELOAD_WORDSTATS=False`

---

## Deployment Steps Used

```bash
# 1. Build new Docker image
cd /home/zeeguu/src/api
docker build -f Dockerfile -t zeeguu/api:latest .

# 2. Apply database migration
mysql -u zeeguu_test -p zeeguu_test < tools/migrations/25-11-04-a--add_cached_tokenization.sql

# 3. Update config
# Edit /home/zeeguu/ops/running/api/api.cfg
# Set PRELOAD_WORDSTATS=False

# 4. Update docker-compose.yml for Elasticsearch
# Add memory limits and ML disable

# 5. Restart all services
cd /home/zeeguu/ops/running/api
docker compose down zapi
docker compose up -d zapi
docker compose up -d elasticsearch_v8

# 6. Monitor logs
docker logs -f api-zapi-1
docker stats
```

---

## Lessons Learned

1. **--preload in Gunicorn is dangerous** with database connections - always let workers initialize their own connections
2. **CPU-bound operations** (like tokenization) should be cached aggressively
3. **Separate cache tables** are better than bloating main tables
4. **Pre-computation at crawl time** eliminates user-facing performance issues
5. **Thread-safe singleton patterns** are critical for shared resources
6. **Elasticsearch 8.x needs explicit memory limits** in containerized environments
7. **Connection pool sizing** is critical for concurrent request handling

---

## Future Considerations

1. Consider mounting code as volume for faster deploys (avoid rebuilding for code changes)
2. Monitor tokenization cache growth over time
3. Consider implementing cache warming for popular articles
4. Monitor Elasticsearch memory usage with 512MB heap
5. Consider adding cache invalidation mechanism if article content changes
6. Add monitoring for connection pool exhaustion
7. Consider implementing BuildKit for faster Docker builds (currently disabled due to missing buildx)

---

## Post-Deployment Discovery: The Real Fix (Evening of 2025-11-04)

### The Continued Investigation

After the demo, the CPU accumulation issue reappeared when testing with the heavily instrumented tokenization code (detailed logging at every step). However, a surprising discovery was made.

### What We Found

When deploying the instrumented code with extensive logging, the API became **completely stable** with CPU staying at 3-5% even under heavy load (multiple homepage refreshes with 15+ concurrent tokenization requests).

### The Key Insight: Container Recreation vs Restart

The critical difference was **how the container was restarted:**

**Before (didn't fix the issue):**
```bash
docker restart api-zapi-1
```
- Stops and starts the same container instance
- Preserves internal state (shared memory, file descriptors, Python process state)
- Some corrupted state survived the restart

**After (fixed the issue):**
```bash
docker compose down zapi
docker compose up -d zapi
```
- Completely destroys the container
- Creates a fresh new container from the image
- All state is cleared - nothing survives

### Root Cause Hypothesis

The CPU accumulation was likely caused by **accumulated internal state** that survived simple container restarts but NOT full container recreation. Possible culprits:

1. **Stanza/PyTorch internal state** - Thread pools or neural network graph state becoming corrupted over time
2. **Shared memory segments** - Stanza uses shared memory for model weights, which might have become corrupted
3. **Python threading state** - Gunicorn worker thread pools accumulating leaked threads
4. **File descriptor leaks** - Slowly accumulating until causing performance degradation

### What Actually Fixed It

The problem was resolved by:
1. **Frontend fix (deployed previously)** - Removed `isTokenizing`, `interactiveSummary`, `interactiveTitle` from React useEffect dependencies (was causing redundant re-renders)
2. **Full container recreation** - Using `docker compose down/up` instead of `docker restart`
3. **Instrumentation side-effect** - Adding timing logs may have inadvertently introduced small delays that prevent race conditions

### Evidence

**Before fix:**
- CPU would climb from 5% to 50%+ per worker
- Continued climbing even after requests completed
- Required manual intervention to recover

**After fix:**
- CPU stable at 3-5% per worker
- Multiple homepage refreshes (15+ concurrent tokenization requests)
- Hours of operation without degradation
- All tokenization requests completing in 1-50ms

### Additional Improvements Made

1. **Cleaned up logging** - Changed high-frequency logs from `logp` to `log` to eliminate duplicates
2. **Removed duplicate teardown** - Removed redundant `teardown_appcontext` handler (was called twice per request)
3. **Fixed deployment scripts** - Added `sleep 5` delays in blue-green deployment health checks to allow API startup time
4. **Upgraded to Stanza 1.11.0** - Latest stable version (though the version wasn't the issue)

### Lessons Learned - Updated

1. **Container restarts are not enough** - Always use `docker compose down/up` for a truly clean restart, not `docker restart`
2. **State accumulation in long-running processes** - Python web workers can accumulate corrupted internal state that survives container restarts
3. **Blue-green deployments mask the issue** - The deployment process naturally does full recreation, which cleared the bad state
4. **Instrumentation can fix bugs** - Adding timing code may introduce delays that prevent race conditions
5. **Frontend bugs can amplify backend issues** - The React useEffect infinite loop was making the backend work much harder

### Recommended Operational Changes

1. **Always use full recreation:** Update all restart procedures to use `docker compose down/up` instead of `docker restart`
2. **Daily container recreation:** Consider a cron job to recreate containers daily as preventive maintenance
3. **Monitor for state accumulation:** Watch for gradual CPU/memory increases that don't correlate with load
4. **Prefer blue-green deployments:** The pink/zapi deployment pattern naturally avoids this issue

### Updated Deployment Best Practices

```bash
# DON'T do this:
docker restart api-zapi-1

# DO this instead:
docker compose down zapi
docker compose up -d zapi

# Or better yet, use the blue-green deployment:
./restart.sh  # Uses pink as staging, then switches
```

---

## Contact / Questions

For questions about these changes, refer to:
- This changelog
- Git commit history for detailed changes
- Docker logs for runtime behavior
- `/CLAUDE.md` and `/api/CLAUDE.md` for project context
